





# pip install imbalanced-learn

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

from scipy.stats.mstats import winsorize
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize
from imblearn.over_sampling import SMOTE

from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score, make_scorer, precision_score, recall_score, f1_score

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier






# statistics and visualization


df = pd.read_csv('Student_dataset.csv')
df.head()


numerical_columns= df.select_dtypes(['int64','float64']).columns
print(f"List of numerical attributes\n")
print(f"{numerical_columns}\n")


# Initialize categories
continuous_features = []
ordinal_features = []
ratio_features = []
binary_features = []

# Categorize features
for col in df.columns:
    unique_vals = df[col].nunique()
    if df[col].dtype in ['float64']:
        if unique_vals == 2:
            binary_features.append(col)  # Binary (Yes/No, Male/Female)
        elif unique_vals > 10:
            if col.lower().endswith(('grade', '(grade)', 'score')):
                continuous_features.append(col)  # Likely Continuous
            else:
                ratio_features.append(col)  # Likely Ratio
        else:
            ordinal_features.append(col)  # Small range, likely Ordinal
    elif df[col].dtype == 'int64':
        if unique_vals == 2:
            binary_features.append(col)  # Binary categorical
        else:
            ordinal_features.append(col)  # Ordinal categorical

# Print categorized features
print("Continuous Features:", continuous_features)
print("Ordinal Features:", ordinal_features)
print("Ratio Features:", ratio_features)
print("Binary Features:", binary_features)



describe = df.describe()
describe


# Combine both for a single visualization layout
float_columns = df.select_dtypes(['float64'])
int_columns = df.select_dtypes(['int64'])
numerical_columns = float_columns + int_columns
all_columns = list(numerical_columns)

# Define the layout
columns_per_row = 3  # Number of plots per row
num_columns = len(all_columns)
num_rows = math.ceil(num_columns / columns_per_row)
# Create subplots
fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(18, num_rows * 5))
axes = axes.flatten()  # Flatten the axes array for easier iteration

# Plot each attribute
for idx, column in enumerate(all_columns):
    ax = axes[idx]
    
    if column in float_columns:
        # Plot histograms with KDE for numerical columns
        sns.histplot(df[column], kde=True, ax=ax, bins=30, color='blue')
        ax.set_title(f"Histogram of {column}")
        ax.set_xlabel(column)
        ax.set_ylabel("Frequency")
    elif column in int_columns:
        # Plot count plots for categorical columns
        sns.countplot(x=df[column], ax=ax, palette="viridis", hue=ax)
        ax.set_title(f"Count of {column}")
        ax.set_xlabel(column)
        ax.set_ylabel("Count")
        # Rotate labels for better visibility
        ax.tick_params(axis='x', rotation=45)
    elif column in categorical_columns:
        # Plot count plots for categorical columns
        sns.countplot(x=df[column], ax=ax, palette="viridis", hue=ax)
        ax.set_title(f"Count of {column}")
        ax.set_xlabel(column)
        ax.set_ylabel("Count")
        # Rotate labels for better visibility
        ax.tick_params(axis='x', rotation=45)

# Hide unused subplots
for idx in range(num_columns, len(axes)):
    axes[idx].axis("off")

# Adjust layout
plt.tight_layout()
plt.show()


# Create subplots
fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(18, num_rows * 5))
axes = axes.flatten()  # Flatten the axes array for easier iteration

# Plot boxplots for numerical columns
for idx, column in enumerate(numerical_columns):
    ax = axes[idx]  # Select the appropriate subplot
    sns.boxplot(x=df[column], ax=ax, color='skyblue')  # Boxplot for each numerical column
    ax.set_title(f"Box Plot of {column}")
    ax.set_xlabel(column)
    ax.grid(axis='x', linestyle='--', alpha=0.7)

# Hide unused subplots
for idx in range(num_columns, len(axes)):
    axes[idx].axis("off")

# Adjust layout
plt.tight_layout()
plt.show()





# Define continuous features (manually or dynamically)

continuous_features = [ 'Previous qualification (grade)',
    'Admission grade',
    'Curricular units 1st sem (grade)',
    'Curricular units 2nd sem (grade)',
    'Unemployment rate',
    'Inflation rate',
    'GDP']  # Replace this with your continuous feature names

# Check if the continuous features exist in the dataset
continuous_features = [col for col in continuous_features if col in df.columns]

# Define the layout
columns_per_row = 3  # Number of plots per row
num_features = len(continuous_features)
num_rows = math.ceil(num_features / columns_per_row)

# Create subplots
fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(18, num_rows * 5))
axes = axes.flatten()  # Flatten the axes array for easier iteration

# Plot boxplots for continuous features
for idx, column in enumerate(continuous_features):
    ax = axes[idx]  # Select the appropriate subplot
    sns.boxplot(x=df[column], ax=ax, color='lightgreen')  # Boxplot for each continuous feature
    ax.set_title(f"Box Plot of {column}")
    ax.set_xlabel(column)
    ax.grid(axis='x', linestyle='--', alpha=0.7)

# Hide unused subplots
for idx in range(num_features, len(axes)):
    axes[idx].axis("off")

# Adjust layout
plt.tight_layout()
plt.show()





#exploring categotical data
categorical_columns = df.select_dtypes(['object']).describe()
categorical_columns


plt.figure()
sns.countplot(  data=df, x='Target', hue='Target', palette='viridis', dodge=False, )
plt.title('Class Distribution for Training Set')
plt.show()





numeric_df = df.select_dtypes(include='number')
corr = numeric_df.corr(method = 'pearson')


corr_threshold = 0.5
filtered_corr = corr[(corr > corr_threshold) | (corr < -corr_threshold)]

plt.figure(figsize=(10, 8), dpi=500)
sns.heatmap(filtered_corr, annot=True, fmt=".2f", linewidths=0.5, cmap="coolwarm", mask=filtered_corr.isnull())
plt.title("Filtered Correlation Heatmap")
plt.show()





# Flatten the correlation matrix and sort
corr_flat = corr.unstack().reset_index()
corr_flat.columns = ['Attribute 1', 'Attribute 2', 'Correlation']
corr_flat['AbsCorrelation'] = corr_flat['Correlation'].abs()

# Remove duplicates (e.g., "Attribute A - Attribute B" and "Attribute B - Attribute A")
corr_flat = corr_flat[corr_flat['Attribute 1'] != corr_flat['Attribute 2']]  # Remove self-correlations
top_10_corr = corr_flat.sort_values(by='AbsCorrelation', ascending=False).head(20)
corr_flat = corr_flat.drop_duplicates(subset=['AbsCorrelation'])

# Sort by absolute correlation and select the top 10
top_10_unique_corr = corr_flat.sort_values(by='AbsCorrelation', ascending=False).head(10)

# Display the top 10 correlations
print("Top 10 Highest Correlations:")
top_10_unique_corr[['Attribute 1', 'Attribute 2', 'Correlation']]


# Plot the Top 10 Correlations
plt.figure(figsize=(12, 6))
sns.barplot(
    x=top_10_unique_corr['Correlation'], 
    y=top_10_unique_corr.apply(lambda x: f"{x['Attribute 1']} - {x['Attribute 2']}", axis=1),
    palette="viridis", hue =top_10_unique_corr.apply(lambda x: f"{x['Attribute 1']} - {x['Attribute 2']}", axis=1)
)
plt.title("Top 10 Highest Correlations Between Attributes")
plt.xlabel("Correlation Coefficient")
plt.ylabel("Attribute Pairs")
plt.tight_layout()
plt.show()





# Create a pivot table for the heatmap
heatmap_data = top_10_corr.pivot(index = "Attribute 1", columns="Attribute 2",values = "Correlation")

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Heatmap of Top 10 Highest Correlations")
plt.tight_layout()
plt.show()











def summarize_columns(dataframe):
    """Summarizes the columns in the dataframe with detailed insights."""
    summary = []
    for column in dataframe.columns:
        dtype = dataframe[column].dtype
        missing_count = dataframe[column].isnull().sum()
        non_missing_count = dataframe[column].notnull().sum()
        unique_values = dataframe[column].nunique()

        if unique_values <= 10:  # If fewer than 10 unique values, show all counts
            values_counts = dataframe[column].value_counts().to_dict()
        else:  # Otherwise, show the top 5 most frequent values
            values_counts = dataframe[column].value_counts().nlargest(5).to_dict()

        summary.append({
            'Column Name': column,
            'Data Type': dtype,
            'Missing Values': missing_count,
            'Non-Missing Values': non_missing_count,
            'Unique Values': unique_values,
            'Top Values (or All if <=10)': values_counts
        })

    return pd.DataFrame(summary)

# Example usage
column_summary = summarize_columns(df)
column_summary











def fillna_by_feature_type(df, feature_types):
    
    for feature_type, features in feature_types.items():
        for feature in features:
            if feature in df.columns:
                if feature_type == 'ratio' or feature_type == 'continuous':
                    # Fill with mean or median
                    if df[feature].dtype in ['float64', 'int64']:
                        df[feature] = df[feature].fillna(df[feature].mean())
                    else:
                        df[feature] = df[feature].fillna(df[feature].median())
                elif feature_type == 'ordinal':
                    # Fill with median or mode
                    mode = df[feature].mode()[0]
                    df[feature] = df[feature].fillna(df[feature].median() if df[feature].dtype in ['float64', 'int64'] else mode)
                elif feature_type == 'binary':
                    # Fill with mode
                    df[feature] = df[feature].fillna(df[feature].mode()[0])
    return df

feature_types = {
    'continuous': continuous_features,
    'ordinal': ordinal_features,
    'ratio': ratio_features,
    'binary': binary_features
}

# Fill missing values
df_filled = fillna_by_feature_type(df, feature_types)





# Define columns to convert
columns_to_convert = ['Application mode', 'Course', "Mother's qualification", 
                      'Displaced', 'Debtor', 'Scholarship holder', 'Age at enrollment']

# Replace NaN with 0 and convert to integer
df_filled[columns_to_convert] = df_filled[columns_to_convert].astype('int64')





after_cleaning = summarize_columns(df_filled)
after_cleaning





df2  = df_filled.copy()
# Check for duplicate rows
duplicates = df2.duplicated().sum()

# Display rows that are duplicates
print(f"Numbers of duplicated Rows: {duplicates}")


after_drop = df2.drop_duplicates()
print(f"Numbers of duplicates after drop {after_drop.shape}")





df3 = after_drop
df3.describe()


# Combine both for a single visualization layout
float_columns = df3.select_dtypes(['float64'])
int_columns = df3.select_dtypes(['int64'])
numerical_columns = float_columns + int_columns
all_columns = list(numerical_columns)

# Define the layout
columns_per_row = 3  # Number of plots per row
num_columns = len(all_columns)
num_rows = math.ceil(num_columns / columns_per_row)
# Create subplots
fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(18, num_rows * 5))
axes = axes.flatten()  # Flatten the axes array for easier iteration

# Plot each attribute
for idx, column in enumerate(all_columns):
    ax = axes[idx]
    
    if column in float_columns:
        # Plot histograms with KDE for numerical columns
        sns.histplot(df3[column], kde=True, ax=ax, bins=30, color='blue')
        ax.set_title(f"Histogram of {column}")
        ax.set_xlabel(column)
        ax.set_ylabel("Frequency")
    elif column in int_columns:
        # Plot count plots for categorical columns
        sns.countplot(x=df3[column], ax=ax, palette="viridis", hue=ax)
        ax.set_title(f"Count of {column}")
        ax.set_xlabel(column)
        ax.set_ylabel("Count")
        # Rotate labels for better visibility
        ax.tick_params(axis='x', rotation=45)
    elif column in categorical_columns:
        # Plot count plots for categorical columns
        sns.countplot(x=df3[column], ax=ax, palette="viridis", hue=ax)
        ax.set_title(f"Count of {column}")
        ax.set_xlabel(column)
        ax.set_ylabel("Count")
        # Rotate labels for better visibility
        ax.tick_params(axis='x', rotation=45)

# Hide unused subplots
for idx in range(num_columns, len(axes)):
    axes[idx].axis("off")

# Adjust layout
plt.tight_layout()
plt.show()





# Assuming 'data' is already loaded and contains the 'Target' column
X = df3.drop('Target', axis=1)
y = df3['Target']
X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel(), test_size=0.3, random_state=0, stratify=y)

# Apply SMOTE for balancing
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# Create a balanced dataset
balanced_data = pd.concat([pd.DataFrame(X_balanced, columns=X.columns), pd.DataFrame(y_balanced, columns=['Target'])], axis=1)

# Save balanced data to a CSV file
balanced_output_path = 'Student_dataset_balanced.csv'
balanced_data.to_csv(balanced_output_path, index=False)
print(f"Balanced data saved to {balanced_output_path}")


# Identify categorical and numerical columns
categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns

print("Categorical Columns:", categorical_columns)
print("Numerical Columns:", numerical_columns)






# Convert X_train back to a DataFrame
X_train_preprocess = pd.DataFrame(X, columns=X.columns)

# Normalize numerical attributes using MinMaxScaler
min_max_scaler = MinMaxScaler()
num_attributes = X_train_preprocess.select_dtypes(['int64', 'float64']).columns  # Identify numerical columns
X_train_preprocess[num_attributes] = min_max_scaler.fit_transform(X_train_preprocess[num_attributes])

# Display the preprocessed DataFrame
X_train_preprocess.head()


#visualize classes after splitting
plt.figure()
sns.countplot(x='Target', data = pd.DataFrame({'Target':y_train}))
plt.title('Class Distribution for Training Set')
plt.show()
plt.figure()
sns.countplot(x='Target', data = pd.DataFrame({'Target':y_test}))
plt.title('Class Distribution for Testing Set')
plt.show()





plt.figure()
sns.countplot(
    data=balanced_data, 
    x='Target', 
    hue='Target',  # Assign 'Target' to hue to match the palette
    palette='viridis',
    dodge=False,  # Ensures bars are not separated
    legend=False  # Removes the legend since it's redundant
)
plt.title('Class Distribution After Balancing', fontsize=14)
plt.xlabel('Class', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()





# logistic regression
# decision trees
# support vector machines (svm)
# k-nearest neighbors (knn)
# random forest











# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the logistic regression model
logreg = LogisticRegression(solver='lbfgs', max_iter=1000)
logreg.fit(X_train_scaled, y_train)

# Check performance on the training set
print('Training accuracy: ', logreg.score(X_train_scaled, y_train))





print('Testing accuracy: ', logreg.score(X_test_scaled, y_test) )





# Predict the labels for the test set
y_pred = logreg.predict(X_test_scaled)

# Evaluate the model
print("Logistic Regression Performance:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# Confusion Matrix
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=logreg.classes_, yticklabels=logreg.classes_)
plt.title("Confusion Matrix for Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


# Train Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000, random_state=42, solver='newton-cg')
logistic_model.fit(X_balanced, y_balanced)

# Predict on the test set
y_pred = logistic_model.predict(X_test)

# Evaluate the model
logistic_report = classification_report(y_test, y_pred, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True)

# Convert the report to a DataFrame
logistic_regression_report_df = pd.DataFrame(logistic_report).transpose()

logistic_regression_report_df


# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
print(conf_mat)

# Check if it's binary classification (2x2 matrix)
if conf_mat.shape == (2, 2):
    tn, fp, fn, tp = conf_mat.ravel()
    print('True positive: ', tp)
    print('True negatives: ', tn)
    print('False positive: ', fp)
    print('False negatives: ', fn)
else:
    # Multi-class classification
    print("Multi-class confusion matrix, cannot unpack directly.")
    for i in range(conf_mat.shape[0]):
        print(f"Metrics for Class {i}:")
        tp = conf_mat[i, i]
        fn = conf_mat[i, :].sum() - tp
        fp = conf_mat[:, i].sum() - tp
        tn = conf_mat.sum() - (tp + fn + fp)
        print(f"  True Positives: {tp}")
        print(f"  True Negatives: {tn}")
        print(f"  False Positives: {fp}")
        print(f"  False Negatives: {fn}")


# Binarize the target variable for multiclass ROC
classes = logreg.classes_
y_test_binarized = label_binarize(y_test, classes=classes)
y_proba = logreg.predict_proba(X_test_scaled)  # Get prediction probabilities

# Plot ROC curve for each class
plt.figure(figsize=(10, 8))
for i, class_label in enumerate(classes):
    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_proba[:, i])
    roc_auc = roc_auc_score(y_test_binarized[:, i], y_proba[:, i])
    plt.plot(fpr, tpr, label=f"Class {class_label} (AUC = {roc_auc:.2f})")

# Plot diagonal line for random chance
plt.plot([0, 1], [0, 1], 'k--', label="Random Chance")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Multiclass ROC Curve")
plt.legend(loc="best")
plt.show()





from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)


#Testing time may take long
print('Training accuracy: ', knn.score(X_train_scaled,y_train))
print('Testing accuracy: ', knn.score(X_test_scaled, y_test))


knn_pred = knn.predict(X_test_scaled)
print(classification_report(y_test, knn_pred))


knn_conf_mat = confusion_matrix(y_test, knn_pred,normalize='true')
sns.heatmap(knn_conf_mat, annot=True)
plt.title("Confusion Matrix for KNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


print(f"Length of y_test: {len(y_test)}")
print(f"Length of knn_pred: {len(knn_pred)}")


# Evaluate the model
knn_report = classification_report(
    y_test, knn_pred, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True
)

# Convert the report to a DataFrame
knn_report_df = pd.DataFrame(knn_report).transpose()

knn_report_df


from sklearn.model_selection import GridSearchCV

# Define parameter grid for K
param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}

# GridSearchCV to find the best K
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

# Best parameters and accuracy
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validated Accuracy:", grid_search.best_score_)

# Evaluate the best model
best_knn = grid_search.best_estimator_
y_pred_best = best_knn.predict(X_test_scaled)
print("\nClassification Report for Best KNN:")
classification_report(y_test, y_pred_best)





from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
svm_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42,)
svm_model.fit(X_train_scaled, y_train)
# Encode labels again if needed
label_encoder = LabelEncoder()
y_pca = label_encoder.fit_transform(y_train)
# Encode labels again if needed


# Make predictions
y_pred = svm_model.predict(X_test_scaled)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))


# Evaluate the model
svm_report = classification_report(y_test, y_pred, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True)

# Convert the report to a DataFrame
svm_report_df = pd.DataFrame(svm_report).transpose()

svm_report_df


print("X_train_scaled shape:", X_train_scaled.shape)
print("X_test_scaled shape:", X_test_scaled.shape)





from sklearn.tree import DecisionTreeClassifier, plot_tree # Import Decision Tree Classifier and plotting function

clf = DecisionTreeClassifier()
clf.get_params()


#First split to get 20% as test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
#Split the training set again to get the validation set (requires calculation the get the needed percentage)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=1)
# 12.5% of 80% is 10% of the whole dataset (0.125 x 0.8 = 0.1)

print('Training set: ', y_train.shape)
print('Validation set: ', y_val.shape)
print('Testing set: ', y_test.shape)


# Train Decision Tree model
decision_tree_model = DecisionTreeClassifier(random_state=42)
decision_tree_model.fit(X_train, y_train)
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# Predict on the test set
y_pred_dt = decision_tree_model.predict(X_test)

# Evaluate the model
decision_tree_report = classification_report(y_test, y_pred_dt, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True)

# Convert the report to a DataFrame
decision_tree_report_df = pd.DataFrame(decision_tree_report).transpose()

decision_tree_report_df


# perform k-fold cross validation (with k=5 or k=10) for all classifiers to ensure robust performance evaluation. 
# report the following metrics based on cross-validation: Accuracy, Precision, Recall, F1-Score





from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier
# Train Random Forest model
random_forest_model = RandomForestClassifier(random_state=42, n_estimators=100)
random_forest_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = random_forest_model.predict(X_test)

# Evaluate the model
rf_report = classification_report(
    y_test, y_pred_rf, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True
)

# Convert the report to a DataFrame
rf_report_df = pd.DataFrame(rf_report).transpose()

rf_report_df








bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), n_estimators=50, random_state=42)
bagging_model.fit(X_train, y_train)
y_pred_bagging = bagging_model.predict(X_test)
bagging_report = classification_report(y_test, y_pred_bagging, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True)
bagging_report_df = pd.DataFrame(bagging_report).transpose()
bagging_report_df





# Train Gradient Boosting model
gradient_boosting_model = GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)
gradient_boosting_model.fit(X_train, y_train)

# Predict on the test set
y_pred_gb = gradient_boosting_model.predict(X_test)

# Evaluate the model
gb_report = classification_report(
    y_test, y_pred_gb, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True
)

# Convert the report to a DataFrame
gb_report_df = pd.DataFrame(gb_report).transpose()

gb_report_df





from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier

# Initialize the encoder
label_encoder = LabelEncoder()

# Fit and transform y_train and y_test
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Train XGBoost model with encoded labels
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train_encoded)

# Predict on the test set
y_pred_xgb = xgb_model.predict(X_test)

# Decode predictions back to original labels for evaluation
y_pred_xgb_decoded = label_encoder.inverse_transform(y_pred_xgb)

# Evaluate the model
xgb_report = classification_report(
    y_test, y_pred_xgb_decoded, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True
)

# Convert the report to a DataFrame
xgb_report_df = pd.DataFrame(xgb_report).transpose()

xgb_report_df





adaboost_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1, random_state=42),
                                    n_estimators=50, learning_rate=1.0, random_state=42)
adaboost_model.fit(X_train, y_train)
y_pred_adaboost = adaboost_model.predict(X_test)
adaboost_report = classification_report(y_test, y_pred_adaboost, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True)
adaboost_report_df = pd.DataFrame(adaboost_report).transpose()
adaboost_report_df





# Adjusted function to handle both dictionary and DataFrame formats
def extract_metrics(report, model_name):
    if isinstance(report, pd.DataFrame):
        # Handle DataFrame format
        model_results[model_name] = {
            'Accuracy': report.loc['accuracy', 'f1-score'] * 100 if 'accuracy' in report.index else 0,
            'Macro Avg Precision': report.loc['macro avg', 'precision'] * 100 if 'macro avg' in report.index else 0,
            'Macro Avg Recall': report.loc['macro avg', 'recall'] * 100 if 'macro avg' in report.index else 0,
            'Macro Avg F1-Score': report.loc['macro avg', 'f1-score'] * 100 if 'macro avg' in report.index else 0
        }
    else:
        # Handle dictionary format
        model_results[model_name] = {
            'Accuracy': report['accuracy'] * 100 if 'accuracy' in report else 0,
            'Macro Avg Precision': report['macro avg']['precision'] * 100 if 'macro avg' in report else 0,
            'Macro Avg Recall': report['macro avg']['recall'] * 100 if 'macro avg' in report else 0,
            'Macro Avg F1-Score': report['macro avg']['f1-score'] * 100 if 'macro avg' in report else 0
        }

# Extract metrics for each model
model_results = {}
extract_metrics(logistic_regression_report_df, 'Logistic Regression')
extract_metrics(decision_tree_report_df, 'Decision Tree')
extract_metrics(svm_report_df, 'SVM')
extract_metrics(knn_report_df, 'KNN')
extract_metrics(rf_report_df, 'Random Forest')
extract_metrics(bagging_report_df, 'Bagging')
extract_metrics(gb_report_df, 'Gradient Boosting')
extract_metrics(xgb_report_df, 'XGBoost')
extract_metrics(adaboost_report_df, 'AdaBoost')

# Convert results to a DataFrame
summary_df_dynamic = pd.DataFrame.from_dict(model_results, orient='index')
summary_df_dynamic





warnings.filterwarnings("ignore", category=FutureWarning)  # Suppress warnings

# Check class distribution
print(df3['Target'].value_counts())

# Prepare features (X) and target (y)
X = df3.drop(columns=['Target'])  # Drop the target column
y = df3['Target']  # Target column

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Update StratifiedKFold to use appropriate n_splits
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Dictionary to store average cross-validation results
cv_results = {}

# Function to perform k-fold cross-validation
def evaluate_model(model, X, y, model_name):
    scoring = {
        'accuracy': 'accuracy',
        'precision': make_scorer(precision_score, average='macro'),
        'recall': make_scorer(recall_score, average='macro'),
        'f1': make_scorer(f1_score, average='macro')
    }
    scores = {}
    for metric, scorer in scoring.items():
        score = cross_val_score(model, X, y, cv=kf, scoring=scorer, n_jobs=-1)  # Parallelized
        scores[metric] = score.mean() * 100  # Convert to percentage
    cv_results[model_name] = scores

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs'),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(kernel='linear', random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'Bagging': BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), n_estimators=50, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),
    'XGBoost': XGBClassifier(random_state=42),
    'AdaBoost': AdaBoostClassifier(
        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),
        n_estimators=50,
        learning_rate=1.0,
        algorithm='SAMME',  # Updated to SAMME
        random_state=42
    )
}

# Evaluate each model
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")
    evaluate_model(model, X_scaled, y_encoded, model_name)  # Use scaled features

# Convert results to DataFrame
cv_results_df = pd.DataFrame(cv_results).transpose()

# Display the results
# Option 1: Print the DataFrame
print(cv_results_df)

# Option 2: Save to a CSV file for further analysis
cv_results_df.to_csv('cv_k-fold_results.csv', index=True)
print("Cross-validation results saved to 'cv_k-fold_results.csv'.")


# Load the cross-validation results from the CSV file
cv_results_df = pd.read_csv('cv_k-fold_results.csv', index_col=0)

# Exclude specified classifiers
excluded_classifiers = ['Bagging', 'Gradient Boosting', 'XGBoost','AdaBoost']
filtered_df = cv_results_df[~cv_results_df.index.isin(excluded_classifiers)]

# Select the metric to be used for the x-axis
x_axis_metric = 'classifiers'  # Options: 'classifiers', 'accuracy', 'precision', 'recall'

# Prepare the x-axis data
if x_axis_metric == 'classifiers':
    x_data = filtered_df.index.tolist()
else:
    x_data = filtered_df[x_axis_metric].tolist()

# Extract data for the plot
accuracy = filtered_df['accuracy'].tolist()
precision = filtered_df['precision'].tolist()
recall = filtered_df['recall'].tolist()

# Create the plot
plt.figure(figsize=(10, 6))

# Plot each metric
plt.plot(x_data, accuracy, marker='o', linestyle='-', label='Accuracy')
plt.plot(x_data, precision, marker='s', linestyle='-', label='Precision')
plt.plot(x_data, recall, marker='^', linestyle='-', label='Recall')

# Add labels, title, and legend
plt.title('Classifiers Performance Comparison (Excluding Some Models)')
plt.xlabel(x_axis_metric.capitalize())
plt.ylabel('% Performance Metrics')
plt.legend(loc='best')

# Display grid for better readability
plt.grid(True)

# Show the plot
plt.tight_layout()
plt.show()


# Extract feature importance
feature_importances = pd.DataFrame({
    'Feature': X_balanced.columns,
    'Importance': random_forest_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importances.head(10), x='Importance', y='Feature')  # Removed `palette`
plt.title('Top 10 Most Important Features (Random Forest)', fontsize=16)
plt.xlabel('Feature Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.tight_layout()
plt.show()

# Display the top features as a DataFrame
print("Top 10 Important Features:")
print(feature_importances.head(40))


# Select the top 10 features based on importance
top_features = feature_importances['Feature'].head(10).values
X_train_top = X_balanced[top_features]
X_test_top = X_test[top_features]

# Train Random Forest model with selected features
rf_model_top_features = RandomForestClassifier(random_state=42, n_estimators=100)
rf_model_top_features.fit(X_train_top, y_balanced)

# Predict on the test set
y_pred_rf_top = rf_model_top_features.predict(X_test_top)

# Evaluate the model
rf_top_report = classification_report(
    y_test, y_pred_rf_top, target_names=['Dropout', 'Enrolled', 'Graduate'], output_dict=True
)

# Convert the report to a DataFrame
rf_top_report_df = pd.DataFrame(rf_top_report).transpose()

# Output the results
rf_top_report_df
